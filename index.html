<html>

  <head>

    <meta charset="UTF-8">

    <title>Audio samples from "StyleTTS: Recombine Disentangled Timbre and Rhythm For Cross Speaker Style Control Speech Synthesis"</title>

    <link rel="stylesheet" type="text/css" href="stylesheet.css"/>

  </head>

  <body>

    <article>

      <header>

        <h1>Audio samples from "StyleTTS: Recombine Disentangled Timbre and Rhythm For Cross Speaker Style Control Speech Synthesis"</h1>

      </header>

    </article>


    <p><b>Authors</b>: Ruimin Wang, Fanbo Meng, Kai Liu, Wei Chen</p>



    <div><b>Abstract:</b>

        </br> We define the speakerâ€™s style as scenes (like news, novels, etc.), dialect accents (like Chinese Henan accent, Chinese Northeast accent) etc. Generally, the style of synthesized speech is closely related to the recording style of the speaker. In this work, we propose StyleTTS, a model that can make a speaker synthesize different styles by learning from other speakers. This model supports the decoupling of rhythm and timbre to attain the goal of cross speaker style control. The model utilizes frame-level VAE to extract prosodic information from speech, then uses two different modules to model timbre and rhythm. During inference time, we combine timbre and rhythm from different speakers to achieve cross speaker style control. Experiments show that our model has good performance in style control, like multiple scene, dialect accent control synthesis, and also on cross-language synthesis. </br></br>

    </div>

    &nbsp;
